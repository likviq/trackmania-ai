{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "CSV_PATH = \"E:\\\\study\\\\bachelor\\\\temp-files\\\\data\\\\Obtain data\\\\dataframes\\\\3_min_training_map.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if not os.path.exists(row['image_path']):\n",
    "        df.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        speed = self.dataframe.iloc[idx]['speed:']\n",
    "        cur_gear = self.dataframe.iloc[idx]['curGear:']\n",
    "        side_speed = self.dataframe.iloc[idx]['sideSpeed:']\n",
    "        input_value = self.dataframe.iloc[idx]['inputSteer:']\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        input_value = round(input_value, 1)\n",
    "        \n",
    "        return image, speed, cur_gear, side_speed, input_value\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_dataset = CustomDataset(dataframe=train_df, transform=transform)\n",
    "test_dataset = CustomDataset(dataframe=test_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\study\\bachelor\\github\\trackmania-ai\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\study\\bachelor\\github\\trackmania-ai\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (vgg16): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=25088, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "        self.vgg16 = nn.Sequential(*list(vgg16.children())[:-1])\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 512),  # Adjust input and output sizes as needed\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1)  # Output a single continuous value\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        extracted_features = self.vgg16(image)\n",
    "\n",
    "        predicted_input = self.head(extracted_features)\n",
    "\n",
    "        return predicted_input\n",
    "\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\study\\bachelor\\github\\trackmania-ai\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "e:\\study\\bachelor\\github\\trackmania-ai\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  1%|          | 1/100 [03:50<6:19:36, 230.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [06:43<11:06:32, 403.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\study\\bachelor\\github\\trackmania-ai\\notebooks\\scripts\\main.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/bachelor/github/trackmania-ai/notebooks/scripts/main.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/bachelor/github/trackmania-ai/notebooks/scripts/main.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m input_value \u001b[39m=\u001b[39m input_value\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/study/bachelor/github/trackmania-ai/notebooks/scripts/main.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m image, speed, cur_gear, side_speed, input_value \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mto(device), speed\u001b[39m.\u001b[39mto(device), cur_gear\u001b[39m.\u001b[39mto(device), side_speed\u001b[39m.\u001b[39mto(device), input_value\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/bachelor/github/trackmania-ai/notebooks/scripts/main.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Обнулення градієнтів\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/bachelor/github/trackmania-ai/notebooks/scripts/main.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(image)  \u001b[39m# Пропускаємо дані через модель\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for batch in train_loader:\n",
    "        image, speed, cur_gear, side_speed, input_value = batch\n",
    "        image = image.to(torch.float32)\n",
    "        input_value = input_value.to(torch.float32)\n",
    "        image, speed, cur_gear, side_speed, input_value = image.to(device), speed.to(device), cur_gear.to(device), side_speed.to(device), input_value.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Обнулення градієнтів\n",
    "        outputs = model(image)  # Пропускаємо дані через модель\n",
    "        # print((outputs, input_value))\n",
    "        loss = criterion(outputs, input_value)  # Обчислюємо втрати\n",
    "        loss.backward()  # Зворотнє поширення помилки\n",
    "        optimizer.step()  # Оновлення ваг моделі\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
